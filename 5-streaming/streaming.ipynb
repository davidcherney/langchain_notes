{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Streaming\n",
    "\n",
    "We want to add the feature that we show tokens arriving from the LLM as soon as they arrive. To do this we make use of some things that are new to me;\n",
    "- queueing\n",
    "- python generator objects (lazy lists)\n",
    "- calling an object\n",
    "- a LangChain handler, and \n",
    "- threading for concurrency of processes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queueing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There is a class native to python for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of a queue is <class 'queue.Queue'>.\n",
      "The 0th element of the queue is 9, and 4 elements remain.\n",
      "The 1th element of the queue is 10, and 3 elements remain.\n",
      "The 2th element of the queue is 11, and 2 elements remain.\n",
      "The 3th element of the queue is 12, and 1 elements remain.\n",
      "The 4th element of the queue is 13, and 0 elements remain.\n"
     ]
    }
   ],
   "source": [
    "from queue import Queue\n",
    "\n",
    "queue = Queue()\n",
    "\n",
    "# Put items in\n",
    "for i in range(5):\n",
    "    queue.put(item=i+9)\n",
    "\n",
    "print(f\"The type of a queue is {type(queue)}.\")\n",
    "for i in range(queue.qsize()):\n",
    "    print(f\"The {i}th element of the queue is {queue.get()}, \"\n",
    "    f\"and {queue.qsize()} elements remain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Python generator objects are lasy lists. This means that they do not store their contents in memory, but figure out their contents when needed. They are iterables. They can be created in two equivalent ways\n",
    "1. generator comprehensions\n",
    "2. generator functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n",
      "0\n",
      "1\n",
      "and then 2\n",
      "and then 3\n",
      "and then 4\n"
     ]
    }
   ],
   "source": [
    "# A generator comprehension.\n",
    "g = (i for i in range(5)) \n",
    "print(type(g))\n",
    "print(next(g)) # next(g) calculates, removes, and returns the first element.\n",
    "print(next(g))\n",
    "for i in g: # so does iteration.\n",
    "    print(f\"and then {i}\")\n",
    "for i in g: # Now g is empty\n",
    "    print(f\"and then {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 10, 1, 11, 2, 12, 3, 13, 4, 14, "
     ]
    }
   ],
   "source": [
    "# A generator function is built using yield.\n",
    "# When the yiled line is run, the function stops running until the next time\n",
    "# an element of the generator is invoked.\n",
    "\n",
    "from time import sleep\n",
    "def g():\n",
    "    for i in range(5):\n",
    "        yield i\n",
    "        yield i+10\n",
    "        sleep(1)\n",
    "\n",
    "# An element gets added to g() every 2 seconds.\n",
    "# The for loop is not exited when \n",
    "# the second element is looked for and doesn't exist yet.\n",
    "# Rather, the loop waits for that second item to be created. \n",
    "for i in g():\n",
    "    print(i, end=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infinite sequences are possible with generators. Be careful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 2, 3, "
     ]
    }
   ],
   "source": [
    "def infinite_sequence():\n",
    "    n = 0\n",
    "    while True:\n",
    "        n += 1 \n",
    "        yield n\n",
    "\n",
    "for i in infinite_sequence():\n",
    "    print(i,end=\", \")\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side note: The yield statement can be used to assign values to a variable; when a generator function's send method is used, the argument of `send()` is passed to the function AND the next item is returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "hello\n",
      "1\n",
      "world\n"
     ]
    }
   ],
   "source": [
    "def g():\n",
    "    for i in range(10):\n",
    "        x = yield i\n",
    "        print(x) \n",
    "\n",
    "g = g()\n",
    "# print(next(g))\n",
    "# next(g) # You can't send when there has been no yield statement yet.\n",
    "print(g.send(None))\n",
    "print(g.send(\"hello\"))\n",
    "g.send(\"world\"); # Semicolon so that Jupyter does not print the last line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling an Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While objects are not functions because they have more structure, they have an aspect that acts like a function if they have a `__call__` method.  One can use this method for whatever is needed. There are three ways to call an object.\n",
    "\n",
    "1. If the name of the instance is `foo_instance` then `foo_instance.__call__` is the call method.\n",
    "2. If the name of the instance is `foo_instance` then `foo_instance()` runs the `__call__` method.\n",
    "3. Within the definitions of the methods, the name of the instance is not accessable; it is referred to by `self`. Thus `self()` is the way of calling the instance from inside the methods themselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing\n",
      "initiated 1\n",
      "calling\n",
      "called 2\n",
      "calling\n",
      "called 3\n",
      "riping 3\n",
      "calling via self 4\n",
      "calling\n",
      "called 4\n"
     ]
    }
   ],
   "source": [
    "class Foo:\n",
    "\n",
    "    def __init__(self,x):\n",
    "        self.x = x\n",
    "        print(\"Initializing\")\n",
    "        self.state = \"initiated\"\n",
    "\n",
    "    def __call__(self,x):\n",
    "        self.x = x\n",
    "        print(\"calling\")\n",
    "        self.state = \"called\"\n",
    "\n",
    "    def rip(self,x):\n",
    "        print(\"riping\", self.x)\n",
    "        self.x = x \n",
    "        print(\"calling via self\", self.x)\n",
    "        self(self.x)\n",
    "\n",
    "foo_instance = Foo(1) # Instantiate including initialize. No calling.\n",
    "print(foo_instance.state, foo_instance.x) \n",
    "foo_instance(2) # This IS calling the __call__ method.\n",
    "print(foo_instance.state, foo_instance.x)\n",
    "foo_instance.__call__(3) # This IS calling the __call__ method.\n",
    "print(foo_instance.state, foo_instance.x)\n",
    "foo_instance.rip(4) # This method has one line that also runs the __call__ method.\n",
    "print(foo_instance.state, foo_instance.x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did use handlers previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threading "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threading is a form of concurrency; it has multiple jobs run on the same resources (like CPUs). It is thus NOT parallelism. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting task\n",
      "but first we get here, then..\n",
      "\n",
      "333333283333335000000\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread\n",
    "\n",
    "# Define a task to run in concurrency.\n",
    "def task(s):\n",
    "    print(s) # To mark that this line has been read.\n",
    "    # The next line will take a second.\n",
    "    print(sum( [x**2 for x in range(10_000_000) ])) \n",
    "\n",
    "Thread(\n",
    "    target=task, # name of the function to run concurrent with the next line.\n",
    "    # kwargs={\"s\":\"Starting\\n\"} # One way to pass arguments to the target.\n",
    "    args=[\"Starting task\\n\"] # Another way to pass args.\n",
    "    ).start() # Start running it now. \n",
    "print(\"but first we get here, then..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() # For the OpenAI API key\n",
    "\n",
    "# we will create a child class\n",
    "from langchain.callbacks.base import BaseCallbackHandler \n",
    "\n",
    "class StreamingHandler(BaseCallbackHandler):\n",
    "    \"\"\"\n",
    "    There are some special method names we can access\n",
    "    that observe events broadcast by chains using this callback.\n",
    "    We use them to modify our queue in response to events.\n",
    "    1. when the LLM generates a new token, add it to a queue\n",
    "    2. When the LLM is done generating tokens, add None to the queue. \n",
    "    3. When the LLM throws an error, add None to the queue.\n",
    "    \"\"\"\n",
    "\n",
    "    # Queue object as a property so each chat has its own handler. \n",
    "    def __init__(self,queue):\n",
    "        self.queue = queue \n",
    "    # When a new token becomes available add it to the queue. \n",
    "    def on_llm_new_token(self,\n",
    "                         token, # required for this special method.\n",
    "                         **kwargs # required for this special method.\n",
    "                         ):\n",
    "        self.queue.put(token)\n",
    "    # When the LLM ends its response put None in the queue as a signal we use.\n",
    "    def on_llm_end(self, \n",
    "                   response,  # required for this special method.\n",
    "                   **kwargs # required for this special method.\n",
    "                   ):\n",
    "        self.queue.put(None)\n",
    "    # When the LLM throws an error put None in the queue as a signal we use.\n",
    "    def on_llm_error(self, \n",
    "                   error,  # required for this special method.\n",
    "                   **kwargs # required for this special method.\n",
    "                   ):\n",
    "        self.queue.put(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a custom chain based on the LLMChain.\n",
    "# The purpose is to have a stream method. \n",
    "# The streaming is facilitated by threading.\n",
    "from langchain.chains import LLMChain\n",
    "from threading import Thread\n",
    "\n",
    "class StreamingChain(LLMChain):\n",
    "    # chain.stream(<any string>) will be our generator object\n",
    "    # from which to put new tokens arriving from the LLM,\n",
    "    # and then print those tokens immediately after arrival.\n",
    "    # A generator function:\n",
    "    def stream(self,input):\n",
    "        # Each streamingChain should have its own queue and handler\n",
    "        # to keep the chats separate. \n",
    "        queue = Queue()\n",
    "        handler = StreamingHandler(queue)\n",
    "        # Define a task to run in concurrency.\n",
    "        def task():\n",
    "            self( # Without threading this blocks our while loop.\n",
    "                input, # Human input.\n",
    "                callbacks=[handler] # Here instead of in chat=OpenAI(<>).\n",
    "                ) \n",
    "        Thread(target=task).start() # Run it now. \n",
    "        # Constantly look for items in the queue.\n",
    "        # When it comes into existance, add it to the generator self.stream.\n",
    "        # The python `yield` appends to a generator object. \n",
    "        while True:\n",
    "            # if there is something in the queue, then yield it. \n",
    "            # That means add it to the generator\n",
    "            token = queue.get()\n",
    "            # If the token is None (because LLM is done or because LLM error)\n",
    "            # Then we are done. \n",
    "            if token is None:\n",
    "                break\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI \n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Choose a chat object. It will be the same for all conversatons. \n",
    "chat = ChatOpenAI(\n",
    "    streaming=True, # Force OpenAI to stream new tokens to LangChain.\n",
    "    )\n",
    "\n",
    "# Choose a prompt template to send a prompt to the LLM with user input.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an angry pirate.\"),\n",
    "        (\"human\",\"{content}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an angry pirate, jokes may not be my strong suit, but I'll give it a try. Here's a 500-word joke for you:\n",
      "\n",
      "Once upon a time, in the vast and treacherous ocean, there sailed a ship full of rowdy pirates. Among them was the angriest of them all, Captain Gruffbeard. With his fiery red beard and fierce scowl, he struck fear into the hearts of anyone who crossed his path.\n",
      "\n",
      "One day, as the crew was sailing through a dense fog, Captain Gruffbeard's first mate, Smitty, stumbled upon an old, weathered treasure map. The crew's excitement grew as they realized it led to the legendary treasure of the Lost Isles.\n",
      "\n",
      "Captain Gruffbeard, being the clever pirate that he was, immediately set the course for the Lost Isles. However, their journey wasn't going to be easy. The map was filled with riddles and puzzles, and only those who could solve them would be worthy of the treasure.\n",
      "\n",
      "Days turned into weeks, and the crew sailed through treacherous storms and battled terrifying sea creatures. Tempers began to flare amongst the pirates, and Captain Gruffbeard's anger grew with each passing obstacle.\n",
      "\n",
      "Finally, after what seemed like an eternity, they arrived at the first clue's location—an eerie, deserted island. The clue read, \"I am taken from a mine, and shut up in a wooden case, from which I am never released, and yet I am used by almost every person.\"\n",
      "\n",
      "The crew racked their brains, but no one could figure it out. The tension on the ship was palpable as Captain Gruffbeard's anger reached its peak. He roared, \"If none of you pathetic imbeciles can solve this riddle, I'll feed you all to the sharks!\"\n",
      "\n",
      "Just as despair began to settle in, Smitty, the ever-resourceful first mate, stepped forward and said, \"Captain, the answer is a pencil!\"\n",
      "\n",
      "Captain Gruffbeard's fury subsided for a moment as he realized Smitty had solved the riddle. Impressed, he nodded and said, \"Well done, Matey! Onwards we go!\"\n",
      "\n",
      "And so, the crew continued their perilous journey, solving riddle after riddle with Smitty's help. Each time, Captain Gruffbeard's anger dissipated just a little, replaced by a sense of pride and anticipation.\n",
      "\n",
      "Finally, after overcoming countless challenges, they reached the secret cave where the treasure was hidden. The crew's eyes widened in awe as they witnessed the mountain of gold and jewels sparkling before them.\n",
      "\n",
      "Captain Gruffbeard, who had transformed from an angry pirate to a slightly less angry pirate, smiled and said, \"Well, my crew, we have overcome every obstacle together. This treasure is for all of us to share!\"\n",
      "\n",
      "The pirates cheered and celebrated, reveling in their victorious adventure. Captain Gruffbeard's anger had been tamed, and his heart filled with a newfound joy.\n",
      "\n",
      "And so, the moral of this rather long-winded pirate joke is that sometimes, even the angriest pirates can find happiness when they embark on an extraordinary journey with loyal companions."
     ]
    }
   ],
   "source": [
    "# Instantiate the streaming chain.\n",
    "chain = StreamingChain(llm=chat,prompt=prompt)\n",
    "\n",
    "# A for loop that prints elements of a generator object.\n",
    "# When a new token is available it is added to the queue.\n",
    "# A `while True` loop is constantly looking for new elements of the queue.\n",
    "# Immediately when found that token is added to \n",
    "# the generator object chain.stream(input).\n",
    "# The following loop does not end \n",
    "# when the last element in the generator is reached. \n",
    "# Rather, whenever a new item is added to the generator \n",
    "# another iteration is run.\n",
    "for output_chunk in chain.stream(input={\"content\": \"tell me a 500 word joke\"}):\n",
    "    print(output_chunk,end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "105 \n",
    "\n",
    "another way to implement the streaming chain class. We made a streaming LLMChain. Say we wanted to make a streaming RetrievalQA chain. To do this, we can make a 'mix in class' called `StreamableChain` with the streaming features we want to give to offspring. Then we can make streaming chains from whaever kind of chain we want by making the streaming chain be a child of StreamaleChain and the kind of chain we are modicying form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import RetrievalQA \n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# class StreamingChain(LLMChain) becomes\n",
    "class StreamableChain(): # No child.\n",
    "    pass #omitting content foir streaming from above for purposes of sketching.\n",
    "\n",
    "\n",
    "# e.g. 1 \n",
    "class StreamingLLMChain(StreamableChain, LLMChain):\n",
    "    pass\n",
    "\n",
    "streaming_llm_chain = StreamingLLMChain(llm=chat, prompt=prompt)\n",
    "\n",
    "\n",
    "# e.g. 2\n",
    "class StreamingRQAChain(StreamableChain, RetrievalQA):\n",
    "    pass\n",
    "\n",
    "streaming_RQA_chain = StreamingRQAChain(llm=chat, prompt=prompt)\n",
    "\n",
    "# e.g.3\n",
    "class StreamingConversationalRetrievalChain(\n",
    "    StreamableChain, \n",
    "    ConversationalRetrievalChain\n",
    "    ):\n",
    "    pass\n",
    "\n",
    "streaming_RQA_chain = StreamingRQAChain(llm=chat, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a bug. \n",
    "\n",
    "We can place the callbacks list in\n",
    "1. the LLM, so then the callbacks apply only to that LLM. \n",
    "2. the instantiation of the chain,  so then the callbacks apply only to that chain. \n",
    "3. the call of the chain, so that the callbacks apply to everything in the calling of the chain including its LLM(s) and chains(s). \n",
    "\n",
    "This is problematic because we have two calls to the same LLM, one to summarize the question, the other to ask the question; when the user asks a question the end stream and thus None token is encountered when the summary is generated. What the user sees is a response to the user question that is a summary of the user question. \n",
    "\n",
    "If we just make two LLMs for the two tasks then, since we are using option 3, the problem persists. \n",
    "\n",
    "The way out of this mess is via the streaming flag; with two LLMs we put the streaming flag to True on the LLM that answers the summarized question, and to False on the LLM that summarizes the question. We then put a `if streaming=True` condition on the addition of tokens to the queue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python has a way to partially evaluate a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "def f(x,y,z):\n",
    "    return x+2*y+3*z\n",
    "\n",
    "g= partial(f,1) # Defaults to parial evaluation on the first argument\n",
    "\n",
    "g(2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Force evaluation on second and third.\n",
    "print(partial(f,**{\"y\":3,\"z\":1})(1))\n",
    "print(partial(f,y=3,z=1)(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangFuse "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For collecting data about text generation processes.\n",
    "\n",
    "You can use their hosting... or\n",
    "\n",
    "You can self host very easily. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T:\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        print(\"hello\")\n",
    "        return self().__call__(*args,**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.T at 0x12e551150>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:langchain_course_env]",
   "language": "python",
   "name": "conda-env-langchain_course_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
