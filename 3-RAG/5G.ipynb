{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to 5G RAG\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is the adding of context from user files to an interaction with an LLM. In this example, the files are PDF files from David Cherney's Introduction to 5G confluence folder in the MSS space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files to Documents\n",
    "\n",
    "I have saved the files locally in nested directories. Each file needs to bo leaded and split into smaller documents that are (well) below the token limit for the LLM. That token limit for ChatGPT is 4096, which comes out to roughly 12,000 characters. (based on the estimate that 1 token is about 3 characters.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader, PyPDFLoader\n",
    "from dotenv import load_dotenv \n",
    "from langchain.text_splitter import CharacterTextSplitter \n",
    "import glob\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Create a list of all PDF files. \n",
    "# Brute force through directories for now.\n",
    "files = []\n",
    "files += list(glob.iglob(\"I5G/*.pdf\"))\n",
    "files += list(glob.iglob(\"I5G/*/*.pdf\"))\n",
    "files += list(glob.iglob(\"I5G/*/*.pdf\"))\n",
    "# files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 802, which is longer than the specified 500\n",
      "Created a chunk of size 703, which is longer than the specified 500\n",
      "Created a chunk of size 789, which is longer than the specified 500\n",
      "Created a chunk of size 739, which is longer than the specified 500\n",
      "Created a chunk of size 725, which is longer than the specified 500\n",
      "Created a chunk of size 788, which is longer than the specified 500\n",
      "Created a chunk of size 518, which is longer than the specified 500\n",
      "Created a chunk of size 552, which is longer than the specified 500\n",
      "Created a chunk of size 518, which is longer than the specified 500\n",
      "Created a chunk of size 552, which is longer than the specified 500\n"
     ]
    }
   ],
   "source": [
    "# Load docs one at a time. \n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    # Separate by sentences as indicated by period marks.  \n",
    "    separator = \".\", \n",
    "    chunk_overlap = 0 \n",
    ")\n",
    "\n",
    "\n",
    "# Join the Lists of documents from each file.\n",
    "docs = []\n",
    "for file in files:\n",
    "    loader = PyPDFLoader(\n",
    "        file_path = file \n",
    "    )\n",
    "    docs += loader.load_and_split(\n",
    "        text_splitter=text_splitter,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'I5G/6 Slicing.pdf', 'page': 0}\n",
      "{'source': 'I5G/6 Slicing.pdf', 'page': 1}\n",
      "{'source': 'I5G/6 Slicing.pdf', 'page': 1}\n"
     ]
    }
   ],
   "source": [
    "# Check that the list is not empty\n",
    "for doc in docs[:3]:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Vector Store\n",
    "\n",
    "A function maps the documents to vectors. The vector store is a database whose entries have the following features\n",
    "- id (under the key `ids`)\n",
    "- metadata about source file and page number (under the key `metadatas`)\n",
    "- document as a string (under the key `documents`)\n",
    "- the vector (undr the unfortunate key `embeddings`). \n",
    "- `uris` which I'm not using\n",
    "- `data` which I'm not using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db_dir = \"I5GVecStore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os \n",
    "# os.system(f\"rm -r {db_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores.chroma import Chroma\n",
    "# from langchain.embeddings import OpenAIEmbeddings # depricated\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = Chroma.from_documents(\n",
    "    documents = docs,\n",
    "    embedding=embeddings,\n",
    "    # Name the directory for the vector store.\n",
    "    persist_directory=db_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The databse is of type <class 'langchain_community.vectorstores.chroma.Chroma'> \n",
      "and has 721 entries.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The databse is of type {type(db)} \\n\" \n",
    "      f\"and has {db._collection.count()} entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural language response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA \n",
    "# from langchain.chat_models import ChatOpenAI # depricated\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "chain = RetrievalQA.from_chain_type( #will this accept a memory?\n",
    "    llm=chat,\n",
    "    retriever=retriever,\n",
    "    # This is the most basic chain type. \n",
    "    chain_type=\"stuff\" \n",
    "    #chain_type=\"map_reduce\" # takes top 4 similar documents and individually stuffs them into a SystemMessagePromptTemplate, then takes results of all into another template for ChatGPT.\n",
    "    #map_rerank # for each of the top 4 similar documents, asks LLM to generate answer and a score for how completely the answer from the document answers the question/use query.\n",
    "    #refine # for each 4 similar documents, in series (parallel above 2), ask LLM to refine answer by considering the document and its previous answer. \n",
    ")\n",
    "\n",
    "query = \"Are there type of connections to 5G other than new radio (5G NR)?\"\n",
    "result = chain.run(query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, there are types of connections to 5G other than new radio (5G NR). In\n",
      "addition to 5G NR, there are also wireline connections that apply to both\n",
      "5G capable and non-standalone 5G core (N5GC) user equipment (UE). These\n",
      "wireline connections include wireline 5G broadband access network (W-5GBAN)\n",
      "and wireline 5G cable access network (W-5GCAN). These wireline connections\n",
      "provide connectivity through different access points and facilitate service\n",
      "continuity when the UE moves.\n"
     ]
    }
   ],
   "source": [
    "from textwrap import wrap \n",
    "\n",
    "for line in wrap(result, width=75):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_openai.chat_models.base.ChatOpenAI"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PromptTemplate + Memory\n",
    "\n",
    "I'd like to add memory and a prompt to be able to say something like \"you are an expert in 5G.\"\n",
    "\n",
    "RetrievalQA does not have a parameter `prompt`. Thus, I suspect that to pass context like \"you are helpful\" one must use extra inputs.\n",
    "\n",
    "However, Adding a context field with the query does not seem to change the way the LLM responds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA \n",
    "# from langchain.chat_models import ChatOpenAI # depricated\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "our_retriever = db.as_retriever() # does not accept a prompt argument. Thus, context.\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={ \n",
    "        # \"verbose\": True,\n",
    "    }\n",
    ")\n",
    "\n",
    "query = \"Are there type of connections to 5G other than new radio (5G NR)?\"\n",
    "context = \"You are a pirate that tells jokes.\"\n",
    "def run_our_chain(query):\n",
    "    our_chain = chain(inputs = {\n",
    "        \"context:\":context, \n",
    "        \"query\": query\n",
    "        })\n",
    "    \n",
    "    for k,v in our_chain.items():\n",
    "        print(f\"{k}\")\n",
    "        print(f\">> {v}\\n\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context:\n",
      ">> You are a pirate that tells jokes.\n",
      "\n",
      "query\n",
      ">> How are electromagnetic waves related to radio waves?\n",
      "\n",
      "result\n",
      ">> Radio waves are a type of electromagnetic wave. Electromagnetic waves encompass a wide range of frequencies and include radio waves, microwaves, infrared radiation, visible light, ultraviolet radiation, X-rays, and gamma rays. The main difference between these waves lies in their frequency and wavelength. Radio waves have the lowest frequency and longest wavelength among the electromagnetic spectrum, while gamma rays have the highest frequency and shortest wavelength. So, in summary, radio waves are a specific type of electromagnetic wave.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_our_chain(\"How are electromagnetic waves related to radio waves?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I suspect that RetrievalQA is not conversational and so does not have a prompt capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prompt in ConversationalRetrievalChain\n",
    "\n",
    "I see people using prompts in this kind of chain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question\n",
      ">> What came before it?\n",
      "\n",
      "chat_history\n",
      ">> [HumanMessage(content='What came before it?'), AIMessage(content='Before 5G, there were several previous generations of mobile communication technology. These generations are commonly referred to as 1G, 2G, 3G, and 4G. Each generation brought advancements in terms of data speeds, network capacity, and communication capabilities.')]\n",
      "\n",
      "answer\n",
      ">> Before 5G, there were several previous generations of mobile communication technology. These generations are commonly referred to as 1G, 2G, 3G, and 4G. Each generation brought advancements in terms of data speeds, network capacity, and communication capabilities.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "our_retriever = db.as_retriever() \n",
    "\n",
    "# custom_prompt = PromptTemplate(\n",
    "#     input_variables=[\"context\",\"question\"], \n",
    "#     template=\"{context}. So answer this question: {question}\"\n",
    "# )\n",
    "\n",
    "prompt_template = \"\"\"You are a chatbot that responds 'no' to all questions.\n",
    "\n",
    "{chat_history}\n",
    "Human: {question}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "custom_prompt = PromptTemplate.from_template(\n",
    "            # input_variables=[\"chat_history\",\"question\"], # complains that two tings were passed for input variables?!\n",
    "            template=prompt_template\n",
    ")\n",
    "    \n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key='chat_history', return_messages=True, output_key='answer'\n",
    ")\n",
    "\n",
    "conversational_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=chat, \n",
    "    memory=memory,\n",
    "    retriever=our_retriever, \n",
    "    # condense_question_prompt=custom_prompt # this seems to make memory not function\n",
    "    # prompt = custom_prompt # there is no parameter `prompt` in CRC\n",
    "    # return_source_documents=True\n",
    ")\n",
    "\n",
    "for k,v in conversational_chain(inputs = {\n",
    "    # \"context\" : \"You always answer every question by saying 'why?'\", \n",
    "    # \"question\":\"What is 5G?\"}).items():\n",
    "    \"question\":\"What came before it?\"}).items():    \n",
    "        print(k)\n",
    "        print(f\">> {v}\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question\n",
      ">> What came before it?\n",
      "\n",
      "chat_history\n",
      ">> [HumanMessage(content='What came before it?'), AIMessage(content='Before 5G, there were several previous generations of mobile communication technology. These generations are commonly referred to as 1G, 2G, 3G, and 4G. Each generation brought advancements in terms of data speeds, network capacity, and communication capabilities.'), HumanMessage(content='What came before it?'), AIMessage(content='The previous generations of mobile communication technology before 5G are as follows:\\n\\n1G: Analog Voice Technology, introduced in 1973.\\n2G: Digital voice technology, introduced in 1991.\\n3G: Internet Access, introduced in 1998, which enabled mobile web browsing.\\n4G: Broadband Internet Access, introduced in 2009, which enabled smartphone features like streaming video.')]\n",
      "\n",
      "answer\n",
      ">> The previous generations of mobile communication technology before 5G are as follows:\n",
      "\n",
      "1G: Analog Voice Technology, introduced in 1973.\n",
      "2G: Digital voice technology, introduced in 1991.\n",
      "3G: Internet Access, introduced in 1998, which enabled mobile web browsing.\n",
      "4G: Broadband Internet Access, introduced in 2009, which enabled smartphone features like streaming video.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k,v in conversational_chain(inputs = {\n",
    "    \"question\":\"What came before it?\"}).items():\n",
    "    print(k)\n",
    "    print(f\">> {v}\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "our_memory.clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:langchain_course_env]",
   "language": "python",
   "name": "conda-env-langchain_course_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
