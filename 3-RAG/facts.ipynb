{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TextLoader\n",
    "\n",
    "The goal of this project is to make an LLM aware of the contents of a document so that \n",
    "- a user can ask questions relevant to that document and \n",
    "- the LLM can respond in terms of the content of that document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install langchain\n",
    "# pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Split a Text file \n",
    "\n",
    "LangChain has several document loaders. Our file, `facts.txt` is a text file, so we use the appropriate loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_community.document_loaders.text.TextLoader'>\n",
      "<class 'list'>\n",
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from dotenv import load_dotenv \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "loader = TextLoader(\"facts.txt\") # an instance of a TextLoader object\n",
    "docs = loader.load() # A list with a single `Document`and with metadata \n",
    "print(type(loader))\n",
    "print(type(docs))\n",
    "print(type(docs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of `docs` is list, however, its elements are of a LangChain specific type and have novel properties:\n",
    "- page content is the content\n",
    "- metadata is about the content. \n",
    "\n",
    "These two properties alone consitute a document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. \"Dreamt\" is the only English word that ends with the letters \"mt.\"\n",
      "2. An ostr...\n",
      "{'source': 'facts.txt'}\n"
     ]
    }
   ],
   "source": [
    "n = 80\n",
    "print(f\"{docs[0].page_content[:n]}...\") # First n characters\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one element in this list, and the `page_content` of that item is the entirety of the contents of the text file. This `page_content` is too long to feed to an LLM; we need to split the text into many documents. \n",
    "\n",
    "Langchain has a text splitter. Beware, its behavior is a little less than intuitive; \n",
    "- chunk first then \n",
    "- look for split character from the end of the chunk moving toward the start of chunk\n",
    "- split where split character is found.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'facts.txt'}\n",
      "1. \"Dreamt\" is the only English word that ends with the letters \"mt.\"\n",
      "2. An ostrich's eye is bigger than its brain.\n",
      "3. Honey is the only natural food that is made without destroying any kind of life.\n",
      "\n",
      "\n",
      "{'source': 'facts.txt'}\n",
      "4. A snail can sleep for three years.\n",
      "5. The longest word in the English language is 'pneumonoultramicroscopicsilicovolcanoconiosis.'\n",
      "6. The elephant is the only mammal that can't jump.\n",
      "\n",
      "\n",
      "{'source': 'facts.txt'}\n",
      "7. The letter 'Q' is the only letter not appearing in any U.S. state name.\n",
      "8. The heart of a shrimp is located in its head.\n",
      "9. Australia is the only continent covered by a single country.\n",
      "\n",
      "\n",
      "{'source': 'facts.txt'}\n",
      "10. The Great Wall of China is approximately 13,171 miles long.\n",
      "11. Bananas are berries, but strawberries aren't.\n",
      "12. The Sphinx of Giza has the body of a lion and the head of a human.\n",
      "\n",
      "\n",
      "{'source': 'facts.txt'}\n",
      "13. The first computer bug was an actual bug trapped in a computer.\n",
      "14. Neil Armstrong was the first man to walk on the moon.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter \n",
    "\n",
    "# Create a text splitter to give to the load_and_split function.\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    is_separator_regex=False, \n",
    "    # First split into chunks of a specified number of characters.\n",
    "    chunk_size = 200, \n",
    "    # Then, split at the separator character that is \n",
    "    # closest to the end of the chunk.\n",
    "    separator = \"\\n\", \n",
    "    # Let chunks to overlap so that content is not mututally disjoint. \n",
    "    chunk_overlap = 0  \n",
    ")\n",
    "\n",
    "loader = TextLoader(\"facts.txt\")\n",
    "docs = loader.load_and_split(\n",
    "    text_splitter=text_splitter,\n",
    ")\n",
    "for doc in docs[:5]:\n",
    "    print(doc.metadata)\n",
    "    print(doc.page_content)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another note on chunking; if the chunk size is set so low that there is no split character in a chunk then a longer chunk is used. A message informs you when this happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 143, which is longer than the specified 130\n",
      "Created a chunk of size 135, which is longer than the specified 130\n",
      "Created a chunk of size 148, which is longer than the specified 130\n",
      "Created a chunk of size 147, which is longer than the specified 130\n",
      "Created a chunk of size 137, which is longer than the specified 130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'facts.txt'}\n",
      "1. \"Dreamt\" is the only English word that ends with the letters \"mt.\"\n",
      "2. An ostrich's eye is bigger than its brain.\n",
      "\n",
      "\n",
      "{'source': 'facts.txt'}\n",
      "3. Honey is the only natural food that is made without destroying any kind of life.\n",
      "4. A snail can sleep for three years.\n",
      "\n",
      "\n",
      "{'source': 'facts.txt'}\n",
      "5. The longest word in the English language is 'pneumonoultramicroscopicsilicovolcanoconiosis.'\n",
      "\n",
      "\n",
      "{'source': 'facts.txt'}\n",
      "6. The elephant is the only mammal that can't jump.\n",
      "7. The letter 'Q' is the only letter not appearing in any U.S. state name.\n",
      "\n",
      "\n",
      "{'source': 'facts.txt'}\n",
      "8. The heart of a shrimp is located in its head.\n",
      "9. Australia is the only continent covered by a single country.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size = 130, \n",
    "    separator = \"\\n\", # does this second by looking for \n",
    "    chunk_overlap = 0 \n",
    ")\n",
    "\n",
    "loader = TextLoader(\"facts.txt\")\n",
    "docs = loader.load_and_split(\n",
    "    text_splitter=text_splitter,\n",
    ")\n",
    "for doc in docs[:5]:\n",
    "    print(doc.metadata)\n",
    "    print(doc.page_content)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk overlap\n",
    "\n",
    "Returning to a reasonable chunk size, lets examine the chunk overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'facts.txt'}\n",
      "1. \"Dreamt\" is the only English word that ends with the letters \"mt.\"\n",
      "2. An ostrich's eye is bigger than its brain.\n",
      "3. Honey is the only natural food that is made without destroying any kind of life.\n",
      "\n",
      "\n",
      "{'source': 'facts.txt'}\n",
      "3. Honey is the only natural food that is made without destroying any kind of life.\n",
      "4. A snail can sleep for three years.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter \n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    is_separator_regex=False,\n",
    "    chunk_size = 200, \n",
    "    separator = \"\\n\", \n",
    "    chunk_overlap = 100 # If this is big enough to capture a separator...\n",
    ")\n",
    "\n",
    "loader = TextLoader(\"facts.txt\")\n",
    "docs = loader.load_and_split(\n",
    "    text_splitter=text_splitter,\n",
    ")\n",
    "for doc in docs[:2]:\n",
    "    print(doc.metadata)\n",
    "    print(doc.page_content)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a chunk overlap of 0 in this project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentenceTransformer runs locally and embeds into $R^{668}$. However, it is from the HuggingFace API so it is blocked by DISH. \n",
    "\n",
    "OpenAI Embeddings is an API and embeds into $R^{1536}$. You have to pay for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image of the query under the OpenAI embedding is of length  1536 and is \n",
      " [0.0017633300633127511, -0.004282372977640875, -0.010506087850271642]...\n"
     ]
    }
   ],
   "source": [
    "# pip install tiktoken # required for some reason\n",
    "import tiktoken\n",
    "# from langchain.embeddings import OpenAIEmbeddings # depricated\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings\n",
    "    )\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "query_string = \"hello there weary traveler\"\n",
    "query_vector = embeddings.embed_query(query_string)\n",
    "print(\"The image of the query under the OpenAI embedding is of length \"\n",
    "      f\" {len(query_vector)} and is \\n {query_vector[:3]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store\n",
    "\n",
    "A vector store is a database of strings and their images; roughly, it is the graph of the embedding. \n",
    "\n",
    "Time to get a database to use as a vector store. LangChain has a version of ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### If ya need it\n",
    "import os \n",
    "os.system(\"rm -r vector-store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The databse is of type <class 'langchain_community.vectorstores.chroma.Chroma'> \n",
      "and has 54 entries.\n"
     ]
    }
   ],
   "source": [
    "# pip install -U langchain chromadb\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter \n",
    "# from langchain.embeddings import OpenAIEmbeddings # depricated\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    is_separator_regex=False, \n",
    "    chunk_size = 200, \n",
    "    separator = \"\\n\", \n",
    "    chunk_overlap = 0  \n",
    ")\n",
    "\n",
    "loader = TextLoader(\"facts.txt\")\n",
    "docs = loader.load_and_split(\n",
    "    text_splitter=text_splitter,\n",
    ")\n",
    "db = Chroma.from_documents(\n",
    "    documents = docs,\n",
    "    embedding=embeddings,\n",
    "    # Name the directory for the vector store.\n",
    "    persist_directory=\"vector-store3\"\n",
    ")\n",
    "print(f\"The databse is of type {type(db)} \\n\" \n",
    "      f\"and has {db._collection.count()} entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The databse is of type <class 'langchain_community.vectorstores.chroma.Chroma'> \n",
      "and has 108 entries.\n"
     ]
    }
   ],
   "source": [
    "db = Chroma.from_documents(\n",
    "    documents = docs,\n",
    "    embedding=embeddings,\n",
    "    # Name the directory for the vector store.\n",
    "    persist_directory=\"vector-store3\"\n",
    ")\n",
    "print(f\"The databse is of type {type(db)} \\n\" \n",
    "      f\"and has {db._collection.count()} entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: The database is now populated. If you run the `.from_documents` method twice without deleting the directory specified by `persist_directory` then you get duplicate entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view individual entries use the get method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids: ['05b8dcae-b61a-11ee-b6ca-acde48001122', '05b8dd1c-b61a-11ee-b6ca-acde48001122']\n",
      "\n",
      "embedding: [-0.0012050693621858954, 0.0028918308671563864, 0.008143449202179909]\n",
      "\n",
      "Metadatas: [{'source': 'facts.txt'}, {'source': 'facts.txt'}]\n",
      "\n",
      "documents: \n",
      "\n",
      "1. \"Dreamt\" is the only English word that ends with the letters \"mt.\"\n",
      "2. An ostrich's eye is bigger than its brain.\n",
      "3. Honey is the only natural food that is made without destroying any kind of life.\n",
      "\n",
      "4. A snail can sleep for three years.\n",
      "5. The longest word in the English language is 'pneumonoultramicroscopicsilicovolcanoconiosis.'\n",
      "6. The elephant is the only mammal that can't jump.\n",
      "\n",
      "uris: None\n",
      "\n",
      "data: None\n"
     ]
    }
   ],
   "source": [
    "# Retrieve two entries with more than the default info included.\n",
    "# Note that `id` is always included. \n",
    "gots = db.get(limit=2, include=['embeddings',\"metadatas\", \"documents\",\n",
    "                        #  \"uris\",\"data\" # we don't use these for local files.\n",
    "                         ])\n",
    "\n",
    "print(f\"ids: {gots['ids']}\\n\")\n",
    "print(f\"embedding: {gots['embeddings'][0][0:3]}\\n\")\n",
    "print(f\"Metadatas: {gots['metadatas']}\\n\")\n",
    "printable_docs = '\\n\\n'.join(gots['documents'][0:2] )\n",
    "print(f\"documents: \\n\\n{printable_docs}\\n\")\n",
    "print(f\"uris: {gots['uris']}\\n\")\n",
    "print(f\"data: {gots['data']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can also retrieve using `ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['05b8dcae-b61a-11ee-b6ca-acde48001122',\n",
       " '05b8dd1c-b61a-11ee-b6ca-acde48001122']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = gots['ids']\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You must set a data loader on the collection if loading from URIs.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m db\u001b[38;5;241m.\u001b[39mget(ids\u001b[38;5;241m=\u001b[39mids, \n\u001b[1;32m      2\u001b[0m     include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadatas\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muris\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_course_env/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:611\u001b[0m, in \u001b[0;36mChroma.get\u001b[0;34m(self, ids, where, limit, offset, where_document, include)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    609\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m include\n\u001b[0;32m--> 611\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_course_env/lib/python3.11/site-packages/chromadb/api/models/Collection.py:203\u001b[0m, in \u001b[0;36mCollection.get\u001b[0;34m(self, ids, where, limit, offset, where_document, include)\u001b[0m\n\u001b[1;32m    200\u001b[0m valid_include \u001b[38;5;241m=\u001b[39m validate_include(include, allow_distances\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m include \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must set a data loader on the collection if loading from URIs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    205\u001b[0m     )\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# We need to include uris in the result from the API to load datas\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m include \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muris\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m include:\n",
      "\u001b[0;31mValueError\u001b[0m: You must set a data loader on the collection if loading from URIs."
     ]
    }
   ],
   "source": [
    "db.get(ids=ids, \n",
    "    include=['embeddings',\"metadatas\", \"documents\",\"uris\",\"data\"]\n",
    "    ) # set a data loader? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use custom ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['1', '10'],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [{'source': 'facts.txt'}, {'source': 'facts.txt'}],\n",
       " 'documents': ['1. \"Dreamt\" is the only English word that ends with the letters \"mt.\"\\n2. An ostrich\\'s eye is bigger than its brain.\\n3. Honey is the only natural food that is made without destroying any kind of life.',\n",
       "  '25. There are about 7,000 feathers on an eagle.\\n26. Marie Curie was the first woman to win a Nobel Prize and remains the only person to have won in two different fields—Physics and Chemistry.'],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\"\"\"Delete existing database, both the object and the directory.\"\"\"\n",
    "# try:\n",
    "#     os.system(\"rm -r vectorstore\")\n",
    "#     del db \n",
    "# except:\n",
    "#     None\n",
    "\n",
    "\n",
    "ids = [str(i) for i in range(1, len(docs) + 1)]\n",
    "\n",
    "\n",
    "db_with_custom_ids = Chroma.from_documents(\n",
    "    documents = docs,\n",
    "    embedding=embeddings,\n",
    "    # persist_directory=\"vstore\",\n",
    "    ids=ids\n",
    ") \n",
    "\n",
    "db_with_custom_ids.get(limit=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there were 54\n",
      "there are 53\n"
     ]
    }
   ],
   "source": [
    "# and we may delete thusly\n",
    "print(f\"there were {db_with_custom_ids._collection.count()}\")\n",
    "db_with_custom_ids._collection.delete(ids=[ids[2]])\n",
    "print(f\"there are {db_with_custom_ids._collection.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search through the documents and find the `k` that are most similar to a query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='1. \"Dreamt\" is the only English word that ends with the letters \"mt.\"\\n2. An ostrich\\'s eye is bigger than its brain.\\n3. Honey is the only natural food that is made without destroying any kind of life.' metadata={'source': 'facts.txt'}\n",
      "page_content='1. \"Dreamt\" is the only English word that ends with the letters \"mt.\"\\n2. An ostrich\\'s eye is bigger than its brain.\\n3. Honey is the only natural food that is made without destroying any kind of life.' metadata={'source': 'facts.txt'}\n",
      "page_content=\"4. A snail can sleep for three years.\\n5. The longest word in the English language is 'pneumonoultramicroscopicsilicovolcanoconiosis.'\\n6. The elephant is the only mammal that can't jump.\" metadata={'source': 'facts.txt'}\n",
      "page_content=\"4. A snail can sleep for three years.\\n5. The longest word in the English language is 'pneumonoultramicroscopicsilicovolcanoconiosis.'\\n6. The elephant is the only mammal that can't jump.\" metadata={'source': 'facts.txt'}\n"
     ]
    }
   ],
   "source": [
    "query = \"What is an interesting fact about the english language?\"\n",
    "\n",
    "results = db.similarity_search(\n",
    "    query = query,\n",
    "    # k = 2 #Number of results to display. Default 4\n",
    "    )\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "{'source': 'facts.txt'}\n",
      "1. \"Dreamt\" is the only English word that ends with the letters \"mt.\"\n",
      "2. An ostrich's eye is bigger than its brain.\n",
      "3. Honey is the only natural food that is made without destroying any kind of life.\n",
      "\n",
      "\n",
      "{'source': 'facts.txt'}\n",
      "1. \"Dreamt\" is the only English word that ends with the letters \"mt.\"\n",
      "2. An ostrich's eye is bigger than its brain.\n",
      "3. Honey is the only natural food that is made without destroying any kind of life.\n",
      "\n",
      "\n",
      "{'source': 'facts.txt'}\n",
      "4. A snail can sleep for three years.\n",
      "5. The longest word in the English language is 'pneumonoultramicroscopicsilicovolcanoconiosis.'\n",
      "6. The elephant is the only mammal that can't jump.\n",
      "\n",
      "\n",
      "{'source': 'facts.txt'}\n",
      "4. A snail can sleep for three years.\n",
      "5. The longest word in the English language is 'pneumonoultramicroscopicsilicovolcanoconiosis.'\n",
      "6. The elephant is the only mammal that can't jump.\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    print(\"\\n\")\n",
    "    print(result.metadata) \n",
    "    print(result.page_content) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A separate method gives the similarity score (cosine similarity) with the similar documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "{'source': 'facts.txt'}\n",
      "Search score: 0.349289208650589\n",
      "1. \"Dreamt\" is the only English word that ends with the letters \"mt.\"\n",
      "2. An ostrich's eye is bigger than its brain.\n",
      "3. Honey is the only natural food that is made without destroying any kind of life.\n",
      "\n",
      "\n",
      "{'source': 'facts.txt'}\n",
      "Search score: 0.35016560554504395\n",
      "1. \"Dreamt\" is the only English word that ends with the letters \"mt.\"\n",
      "2. An ostrich's eye is bigger than its brain.\n",
      "3. Honey is the only natural food that is made without destroying any kind of life.\n",
      "\n",
      "\n",
      "{'source': 'facts.txt'}\n",
      "Search score: 0.3518882989883423\n",
      "4. A snail can sleep for three years.\n",
      "5. The longest word in the English language is 'pneumonoultramicroscopicsilicovolcanoconiosis.'\n",
      "6. The elephant is the only mammal that can't jump.\n",
      "\n",
      "\n",
      "{'source': 'facts.txt'}\n",
      "Search score: 0.35238325595855713\n",
      "4. A snail can sleep for three years.\n",
      "5. The longest word in the English language is 'pneumonoultramicroscopicsilicovolcanoconiosis.'\n",
      "6. The elephant is the only mammal that can't jump.\n"
     ]
    }
   ],
   "source": [
    "results = db.similarity_search_with_score(\n",
    "    \"What is an interesting fact about the english language?\",\n",
    "    # k = 2 #Number of results to display. Default 4\n",
    "    )\n",
    "\n",
    "for result in results:\n",
    "    print(\"\\n\")\n",
    "    print(result[0].metadata) \n",
    "    print(f\"Search score: {result[1]}\") #search score\n",
    "    print(result[0].page_content) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt \n",
    "\n",
    "We returned chunks previously. That was awkward because it was not natural language.\n",
    "\n",
    "## RetrievalQA \n",
    "\n",
    "`RetrievalQA` is a chain for searching a vector store for relevant entries, and presenting results with a string generated by an LLM. \n",
    "\n",
    "It requires a retriever object. Retriever objects deserve some explaining. \n",
    "\n",
    "Our vector store `db`  has the method `.similarity_search` that you might think `RetrievalQA` could use to get relevant documents from the vector store before the LLM decides how to present the result. But that method name is specific to Chroma; \n",
    "LangChain's devs realized that there would be a huge number \n",
    "of vector store options some day. So, instead of having a RetrievalQA chain \n",
    "that has code for every known vector store's version of `similarity search`\n",
    "they decided that devs of vector stores, like Chroma, need to provide an \n",
    "`.as_retriever` object on their vector store with the method \n",
    "```python\n",
    "db.get_relevant_documents(s: string) -> documents\n",
    "```\n",
    "to allow the vector store and RetrievalQA to \"talk\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA \n",
    "# from langchain.chat_models import ChatOpenAI #depricated \n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\"\"\"For debuggin, use these two lines. \n",
    "Warning: verbose and hard to read.\"\"\"\n",
    "# import langchain\n",
    "# langchain.debug = True \n",
    "\n",
    "load_dotenv() \n",
    "chat = ChatOpenAI()\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "db = Chroma(\n",
    "    persist_directory = \"emb\",\n",
    "    # embedding # dont use this so that new vectors are not put into the database. \n",
    "    embedding_function = embeddings, # use this \n",
    ")\n",
    "\n",
    "# Chroma has complied and has a .as_retriever method on its database objects.\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    retriever=retriever,\n",
    "    # `chain_type`s are are loaded topic. More later.\n",
    "    #  the argument `stuff` just stuffs all of the retrieved documents \n",
    "    # into a SystemMessagePromptTemplate. \n",
    "    # This is the most basic chain type. \n",
    "    chain_type=\"stuff\",\n",
    "    # Other chain_type argument options:\n",
    "    #chain_type=\"map_reduce\" # takes top 4 similar documents and individually stuffs them into a SystemMessagePromptTemplate, then takes results of all into another template for ChatGPT.\n",
    "    #map_rerank # for each of the top 4 similar documents, asks LLM to generate answer and a score for how completely the answer from the document answers the question/use query.\n",
    "    #refine # for each 4 similar documents, in series (parallel above 2), ask LLM to refine answer by considering the document and its previous answer. \n",
    ")\n",
    "\n",
    "query = \"Give me an interesting fact about the english language.\"\n",
    "result = chain.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of running the chain with the query is as follows:\n",
      "---\n",
      "An interesting fact about the English language is that it is constantly evolving and adding new words. On average, around 1,000 new words are added to the English dictionary each year.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "print(f\"The result of running the chain with the query is as follows:\\n---\\n{result}\\n---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ME: For fun, a function to wrap text. I discovered later that there is such a think in the python default libraries. \n",
    "```python\n",
    "from textwrap import wrap \n",
    "for line in wrap(result, width=75):\n",
    "    print(line)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wrap(s, max_char = 60):\n",
    "    lines =[]\n",
    "    position=0\n",
    "    while position < len(s):\n",
    "        candidate_line = s[position:position + max_char]\n",
    "        reversed = candidate_line[::-1]\n",
    "        reversed_first_space = reversed.index(\" \")\n",
    "        last_space = max_char-1-reversed_first_space\n",
    "        position += last_space + 1\n",
    "        line= candidate_line[:last_space+1]\n",
    "        lines.append(line)\n",
    "    for line in lines:\n",
    "        print(line)\n",
    "    return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. \"Dreamt\" is the only English word that ends with the \n",
      "letters \"mt.\"\n",
      "2. An ostrich's eye is bigger than its \n",
      "brain.\n",
      "3. Honey is the only natural food that is made \n",
      "without destroying any kind of life.\n",
      "4. A snail can sleep \n",
      "for three years.\n",
      "5. The longest word in the English \n",
      "language is \n",
      "'pneumonoultramicroscopicsilicovolcanoconiosis.'\n",
      "6. The \n",
      "elephant is the only mammal that can't jump.\n",
      "7. The letter \n",
      "'Q' is the only letter not appearing in any U.S. state \n",
      "name.\n",
      "8. The heart of a shrimp is located in its head.\n",
      "9. \n",
      "Australia is the only continent covered by a single \n",
      "country.\n",
      "10. The Great Wall of China is approximately \n",
      "13,171 miles long.\n",
      "11. Bananas are berries, but \n",
      "strawberries aren't.\n",
      "12. The Sphinx of Giza has the body of \n",
      "a lion and the head of a human.\n",
      "13. The first computer bug \n",
      "was an actual bug trapped in a computer.\n",
      "14. Neil Armstrong \n",
      "was the first man to walk on the moon.\n",
      "15. The Eiffel Tower \n",
      "in Paris leans slightly in the sun due to thermal \n",
      "expansion.\n",
      "16. Queen Elizabeth II is the longest-reigning \n",
      "current monarch.\n",
      "17. The Leaning Tower of Pisa took 200 \n",
      "years to construct.\n",
      "18. Angel Falls is the highest \n",
      "waterfall in the world, located in Venezuela.\n",
      "19. Sword \n",
      "swallowing is a skill that takes 3-10 years to learn.\n",
      "20. \n",
      "Isaac Newton invented the cat flap.\n",
      "21. Earth, Texas, is \n",
      "the only place on Earth named 'Earth.'\n",
      "22. Thomas Edison, \n",
      "who invented the lightbulb, was afraid of the dark.\n",
      "23. The \n",
      "Pacific Ocean is the largest ocean on Earth, covering more \n",
      "than 60 million square miles.\n",
      "24. Zeus was the king of the \n",
      "Greek gods according to ancient Greek myth.\n",
      "25. There are \n",
      "about 7,000 feathers on an eagle.\n",
      "26. Marie Curie was the \n",
      "first woman to win a Nobel Prize and remains the only \n",
      "person to have won in two different fields—Physics and \n",
      "Chemistry.\n",
      "27. The Sahara Desert is the largest hot desert \n",
      "in the world.\n",
      "28. There are 86,400 seconds in a day.\n"
     ]
    }
   ],
   "source": [
    "s= \"\"\"\n",
    "1. \"Dreamt\" is the only English word that ends with the letters \"mt.\"\n",
    "2. An ostrich's eye is bigger than its brain.\n",
    "3. Honey is the only natural food that is made without destroying any kind of life.\n",
    "4. A snail can sleep for three years.\n",
    "5. The longest word in the English language is 'pneumonoultramicroscopicsilicovolcanoconiosis.'\n",
    "6. The elephant is the only mammal that can't jump.\n",
    "7. The letter 'Q' is the only letter not appearing in any U.S. state name.\n",
    "8. The heart of a shrimp is located in its head.\n",
    "9. Australia is the only continent covered by a single country.\n",
    "10. The Great Wall of China is approximately 13,171 miles long.\n",
    "11. Bananas are berries, but strawberries aren't.\n",
    "12. The Sphinx of Giza has the body of a lion and the head of a human.\n",
    "13. The first computer bug was an actual bug trapped in a computer.\n",
    "14. Neil Armstrong was the first man to walk on the moon.\n",
    "15. The Eiffel Tower in Paris leans slightly in the sun due to thermal expansion.\n",
    "16. Queen Elizabeth II is the longest-reigning current monarch.\n",
    "17. The Leaning Tower of Pisa took 200 years to construct.\n",
    "18. Angel Falls is the highest waterfall in the world, located in Venezuela.\n",
    "19. Sword swallowing is a skill that takes 3-10 years to learn.\n",
    "20. Isaac Newton invented the cat flap.\n",
    "21. Earth, Texas, is the only place on Earth named 'Earth.'\n",
    "22. Thomas Edison, who invented the lightbulb, was afraid of the dark.\n",
    "23. The Pacific Ocean is the largest ocean on Earth, covering more than 60 million square miles.\n",
    "24. Zeus was the king of the Greek gods according to ancient Greek myth.\n",
    "25. There are about 7,000 feathers on an eagle.\n",
    "26. Marie Curie was the first woman to win a Nobel Prize and remains the only person to have won in two different fields—Physics and Chemistry.\n",
    "27. The Sahara Desert is the largest hot desert in the world.\n",
    "28. There are 86,400 seconds in a day.\"\"\"\n",
    "wrap(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Chain Types\n",
    "\n",
    "- chain_type=\"map_reduce\"  takes top 4 similar documents and individually stuffs them into a SystemMessagePromptTemplate, then takes results of all into another template for ChatGPT.\n",
    "- map_rerank .., for each of the top 4 similar documents, asks LLM to generate answer and a score for how completely the answer from the document answers the question/use query.\n",
    "- refine ... for each 4 similar documents, in series (parallel above 2), ask LLM to refine answer by considering the document and its previous answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Redundat Files \n",
    "\n",
    "A vector store will contain duplicate entries; two entries with different ids can have the same string AND the same image of the string under the embedding. \n",
    "\n",
    "You might say that the vector store is then an indexed graph of the embedding with possible duplicate entries. \n",
    "\n",
    "Returning the same entry twice is not desirable because the retriever will retrieve multiple compies of the most relevant document, squeezing out the 5th most important unique document. \n",
    "\n",
    "\n",
    "We can force having duplicate entries by running this code two or more times. \n",
    "\n",
    "```python\n",
    "db = Chroma.from_documents(\n",
    "    documents = docs,\n",
    "    embedding=embeddings,\n",
    "    # Name the directory for the vector store.\n",
    "    persist_directory=\"vector-store\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to make sure there are more that 54 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db._collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that similarity search returns redundancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='1. \"Dreamt\" is the only English word that ends with the letters \"mt.\"\\n2. An ostrich\\'s eye is bigger than its brain.\\n3. Honey is the only natural food that is made without destroying any kind of life.', metadata={'source': 'facts.txt'}),\n",
       "  0.34984490275382996),\n",
       " (Document(page_content='1. \"Dreamt\" is the only English word that ends with the letters \"mt.\"\\n2. An ostrich\\'s eye is bigger than its brain.\\n3. Honey is the only natural food that is made without destroying any kind of life.', metadata={'source': 'facts.txt'}),\n",
       "  0.3509979844093323),\n",
       " (Document(page_content=\"4. A snail can sleep for three years.\\n5. The longest word in the English language is 'pneumonoultramicroscopicsilicovolcanoconiosis.'\\n6. The elephant is the only mammal that can't jump.\", metadata={'source': 'facts.txt'}),\n",
       "  0.3523455858230591),\n",
       " (Document(page_content=\"4. A snail can sleep for three years.\\n5. The longest word in the English language is 'pneumonoultramicroscopicsilicovolcanoconiosis.'\\n6. The elephant is the only mammal that can't jump.\", metadata={'source': 'facts.txt'}),\n",
       "  0.3527907431125641)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = db.similarity_search_with_score(\n",
    "    \"What is an interesting fact about the english language?\",\n",
    "    # k = 2 #Number of results to display. Default 4\n",
    "    )\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Margnal Relevance Searches\n",
    "\n",
    "There are two methods on db that we can use to remove redundancies; both work by returning the (unique) documents with the 4 highest relevance scores. \n",
    "- one takes in a string\n",
    "- the other takes in the vector that is the image of that string under the embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='1. \"Dreamt\" is the only English word that ends with the letters \"mt.\"\\n2. An ostrich\\'s eye is bigger than its brain.\\n3. Honey is the only natural food that is made without destroying any kind of life.', metadata={'source': 'facts.txt'}),\n",
       " Document(page_content=\"4. A snail can sleep for three years.\\n5. The longest word in the English language is 'pneumonoultramicroscopicsilicovolcanoconiosis.'\\n6. The elephant is the only mammal that can't jump.\", metadata={'source': 'facts.txt'}),\n",
       " Document(page_content=\"86. Broccoli and cauliflower are the only vegetables that are flowers.\\n87. The dot over an 'i' or 'j' is called a tittle.\\n88. A group of owls is called a parliament.\", metadata={'source': 'facts.txt'}),\n",
       " Document(page_content='118. The original Star-Spangled Banner was sewn in Baltimore.\\n119. The average adult spends more time on the toilet than they do exercising.', metadata={'source': 'facts.txt'})]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_string = \"what is an interesting fact about the english language?\"\n",
    "\n",
    "db.max_marginal_relevance_search(\n",
    "    query = query_string,\n",
    "    lambda_mult=0.8\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='1. \"Dreamt\" is the only English word that ends with the letters \"mt.\"\\n2. An ostrich\\'s eye is bigger than its brain.\\n3. Honey is the only natural food that is made without destroying any kind of life.', metadata={'source': 'facts.txt'}),\n",
       " Document(page_content=\"4. A snail can sleep for three years.\\n5. The longest word in the English language is 'pneumonoultramicroscopicsilicovolcanoconiosis.'\\n6. The elephant is the only mammal that can't jump.\", metadata={'source': 'facts.txt'}),\n",
       " Document(page_content=\"86. Broccoli and cauliflower are the only vegetables that are flowers.\\n87. The dot over an 'i' or 'j' is called a tittle.\\n88. A group of owls is called a parliament.\", metadata={'source': 'facts.txt'}),\n",
       " Document(page_content='118. The original Star-Spangled Banner was sewn in Baltimore.\\n119. The average adult spends more time on the toilet than they do exercising.', metadata={'source': 'facts.txt'})]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_vector = embeddings.embed_query(query_string)\n",
    "\n",
    "db.max_marginal_relevance_search_by_vector(\n",
    "    embedding = query_vector,\n",
    "    lambda_mult=0.8\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this is not natural language. Thus, we would like to combine `RetrievalQA` with this kind of search, thereby \n",
    "- reducing the amount of tokens sent by reducing redundancy\n",
    "- having the LLM consider 4 distinct documents instead of repeat documents. \n",
    "\n",
    "### Custom File Retriever\n",
    "\n",
    "To combine these ingredients we create our own retriever. In the file `redundant_file_retriever.py` we specify a retriever object with the following:\n",
    "```python \n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.schema import BaseRetriever\n",
    "\n",
    "class RedundantFilterRetriever(BaseRetriever):\n",
    "    \"\"\"\n",
    "    Vector stores are required by LangChain to have an oject that functions \n",
    "    as a retriever and has two methods \n",
    "    (get_relevant_documents and aget_relevant_documents).\n",
    "    Instead of using the built in object for Chroma, we are making a custom \n",
    "    retriever object.\n",
    "    \"\"\"\n",
    "    #    Require the instantiator to \n",
    "    # 1. specify an embedding (named `embeddings`) that is the class Embeddings\n",
    "    # 2. specify a vector store (named `chroma`) that is in the class Chroma\n",
    "    embeddings: Embeddings \n",
    "    chroma: Chroma\n",
    "\n",
    "    def get_relevant_documents(self, query_string ):\n",
    "        \"\"\"\n",
    "        This method is required for any retriever object \n",
    "        \"\"\"\n",
    "        # Calculate image of query.\n",
    "        query_vector = self.embeddings.embed_query(query_string)\n",
    "        # Feed image to max_marginal_relevance_search_by_vector \n",
    "        results = self.chroma.max_marginal_relevance_search_by_vector(\n",
    "            embedding = query_vector, # the parameter name is LangChain's fault. \n",
    "            lambda_mult=0.8 \n",
    "            )\n",
    "        return results\n",
    "    \n",
    "    async def aget_relevant_documents(self):\n",
    "        return []\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call Custom File Retriever\n",
    "\n",
    "We have to instantiate the object before calling a method on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest word in the English language is 'pneumonoultramicroscopicsilicovolcanoconiosis.'\n"
     ]
    }
   ],
   "source": [
    "from redundant_filter_retriever import RedundantFilterRetriever\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "embeddings = OpenAIEmbeddings() #Remember that this is what we are using.\n",
    "\n",
    "# create a db without adding entries. \n",
    "db_from_file = Chroma(\n",
    "    persist_directory = \"vector-store\",\n",
    "    # embedding # dont use this so that new vectors are not put into the database. \n",
    "    embedding_function = embeddings, # use this; the name sucks IMO. \n",
    ")\n",
    "\n",
    "# Instantiate our custom retriever. \n",
    "retriever = RedundantFilterRetriever(\n",
    "    embeddings = embeddings,\n",
    "    chroma = db_from_file\n",
    ")\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\" \n",
    ")\n",
    "\n",
    "query = \"Give me an interesting fact about the english language.\"\n",
    "result = chain.run(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the LLM has given us a single fact in natural language. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:langchain_course_env]",
   "language": "python",
   "name": "conda-env-langchain_course_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
