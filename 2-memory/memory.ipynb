{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory\n",
    "\n",
    "ChatGPT does not remember your previous questions; it is sent a summary of your previous conversation together with your current message. This project demonstrates how that is done. \n",
    "\n",
    "Note that there are two kinds of LLMs;\n",
    "1. completion models aim to auto-complete a statement you began.\n",
    "2. chat models aim to mimic a conversation between user and LLM.\n",
    "\n",
    "Chat models are actually a form of completion model, but the distinction is useful. In either case, the LLM does not retain memory of past insterctions. \n",
    "\n",
    "Note also that the devlopers of LangChain seem to anticipate completion models being dominant over chat models in the future. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Buffer Chat History \n",
    "\n",
    "The class `ConversationBufferMemory` is a tool for putting conversation history into a file. The file is read and put into the prompt \n",
    "```\n",
    "You are a chatbot having a conversation with a human.\n",
    "\n",
    "{chat_history}\n",
    "Human: {human_input}\n",
    "Chatbot:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    #  \n",
    "    memory_key=\"messages\", \n",
    "    # Don't just throw strings into that history,\n",
    "    # encase the strings in \"HumanMessage()\" and \"AIMessage()\".\n",
    "    return_messages = True, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct a chain and see this memory in action. To start, we will use the familiar `PromptTemplate`, then we will move on to the more sophisticated `ChatPromptTemplate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Call 1 --- \n",
      ">>human_input<<\n",
      "  What is 1+1?\n",
      ">>chat_history<<\n",
      "  [HumanMessage(content='What is 1+1?', additional_kwargs={}, example=False), AIMessage(content='1+1 equals 2.', additional_kwargs={}, example=False)]\n",
      ">>text<<\n",
      "  1+1 equals 2.\n",
      "\n",
      "\n",
      "+++++++Call 1 Chat History:++++++\n",
      ":Message 0 is: \n",
      "content='What is 1+1?' additional_kwargs={} example=False\n",
      ":Message 1 is: \n",
      "content='1+1 equals 2.' additional_kwargs={} example=False\n",
      "--- Call 2 --- \n",
      ">>human_input<<\n",
      "   And 3 more than that?\n",
      ">>chat_history<<\n",
      "   [HumanMessage(content='What is 1+1?', additional_kwargs={}, example=False), AIMessage(content='1+1 equals 2.', additional_kwargs={}, example=False), HumanMessage(content='And 3 more than that?', additional_kwargs={}, example=False), AIMessage(content='3 more than 2 is 5.', additional_kwargs={}, example=False)]\n",
      ">>text<<\n",
      "   3 more than 2 is 5.\n",
      "\n",
      "\n",
      "+++++++Call 2 Chat History:++++++\n",
      ":Message 0 is: \n",
      "content='What is 1+1?' additional_kwargs={} example=False\n",
      ":Message 1 is: \n",
      "content='1+1 equals 2.' additional_kwargs={} example=False\n",
      ":Message 2 is: \n",
      "content='And 3 more than that?' additional_kwargs={} example=False\n",
      ":Message 3 is: \n",
      "content='3 more than 2 is 5.' additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI \n",
    "from langchain.chains import LLMChain \n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "\n",
    "template = \"\"\"You are a chatbot having a conversation with a human.\n",
    "\n",
    "{chat_history}\n",
    "Human: {human_input}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"human_input\"], \n",
    "    template=template\n",
    "    )\n",
    "\n",
    "our_memory = ConversationBufferMemory(\n",
    "    # Matcing the key \"chat_history\" here with that in prompt \n",
    "    # makes the memory the input variable\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages = True,\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt= prompt,\n",
    "    memory=our_memory # This overrides the default of no memory.\n",
    ")\n",
    "result1 = chain({\n",
    "        \"human_input\":\"What is 1+1?\"\n",
    "        })\n",
    "\n",
    "print(\"--- Call 1 --- \")\n",
    "for k,v in result1.items():\n",
    "    print(f\">>{k}<<\")\n",
    "    print(f\"  {v}\")\n",
    "\n",
    "print(\"\\n\\n+++++++Call 1 Chat History:++++++\")\n",
    "for i, message in enumerate(result1[\"chat_history\"]):\n",
    "    print(f\":Message {i} is: \\n{message}\")\n",
    "    \n",
    "result2 = chain({\n",
    "        \"human_input\":\"And 3 more than that?\"\n",
    "        })\n",
    "\n",
    "\n",
    "print(\"--- Call 2 --- \")\n",
    "for k,v in result2.items():\n",
    "    print(f\">>{k}<<\")\n",
    "    print(f\"   {v}\")\n",
    "\n",
    "print(\"\\n\\n+++++++Call 2 Chat History:++++++\")\n",
    "for i, message in enumerate(result2[\"chat_history\"]):\n",
    "    print(f\":Message {i} is: \\n{message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatPromptTemplate is a template for \n",
    "putting strings that provide context \n",
    "into a message. That input can be \n",
    "- system message content (which preceeds a conversation and is often used to inform the LLM if its role as, say, a helpful chat bot)\n",
    "- messages placeholder, which is a tool for putting in a list of messages\n",
    "- a human message.\n",
    "\n",
    "It is introduced here because it is a way to introduce the previous exchange as context for the current user input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='What is 1+1?' additional_kwargs={} example=False\n",
      "content=\"I don't know and I don't care! Math is boring and I don't want to answer your stupid questions!\" additional_kwargs={} example=False\n",
      "content='And 3 more than that?' additional_kwargs={} example=False\n",
      "content=\"I told you, I don't want to do math! Why don't you ask someone else who actually cares about this stuff?\" additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI \n",
    "from langchain.chains import LLMChain \n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "chat = ChatOpenAI() \n",
    "\n",
    "prompt = ChatPromptTemplate( \n",
    "    # Input the message history and the new user content.\n",
    "    # These together are the input variables. \n",
    "    input_variables = [ \n",
    "        # Include the new message to the LM.\n",
    "        \"human_input\",\n",
    "        # Include the message history in buffer memory\n",
    "        \"chat_history\"\n",
    "        ],\n",
    "    # List of things that are to be sent to the LLM.\n",
    "    messages=[ \n",
    "        SystemMessage(content=\"\"\"\n",
    "                      You are a frustrated kindergarten student \n",
    "                      that refuses to cooperate with your teacher, \n",
    "                      who is asking you questions about math.\n",
    "                      \"\"\" \n",
    "                      ),\n",
    "        MessagesPlaceholder(\n",
    "            variable_name=\"chat_history\"\n",
    "            ),\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"\"\"Answer this question about math: {human_input}\"\"\"\n",
    "            )\n",
    "    ]\n",
    ")\n",
    "\n",
    "our_memory = ConversationBufferMemory( # should clear the buffer chat history from previous sections.\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages = True,\n",
    "    )\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=prompt,\n",
    "    memory=ourmemory # This overrides the default of no memory.\n",
    "    )\n",
    "result = chain({\n",
    "        \"human_input\":\"What is 1+1?\"\n",
    "        })\n",
    "result = chain({\n",
    "        \"human_input\":\"And 3 more than that?\"\n",
    "        })\n",
    "\n",
    "# for k,v in result.items():\n",
    "#     print(f\">>{k}<<\")\n",
    "#     print(v)\n",
    "\n",
    "for message in result[\"chat_history\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I note nere that the system message is not included in the memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File chat history\n",
    "\n",
    "Instances of the object `ConversationBufferMemory` can write the message history to a JSON file using the parameter `chat_memory` with argument the an instance of the object `FileChatMessageHistory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>human_input<<\n",
      "  Do you like fish?\n",
      "\n",
      "+++++ Mesages: ++++++\n",
      "\n",
      ">>messages<<\n",
      "  [HumanMessage(content='What is your favorite food?', additional_kwargs={}, example=False), AIMessage(content=\"Meow! As a hungry cat, I have a few favorite foods. I absolutely love fish, especially salmon and tuna. But I also enjoy some yummy chicken and even the occasional treat of catnip! What about you? What's your favorite food?\", additional_kwargs={}, example=False)]\n",
      "\n",
      "+++++ Mesages: ++++++\n",
      "\n",
      ">>text<<\n",
      "  Oh, absolutely! Fish is definitely one of my favorite foods. Whether it's fresh or cooked, I can't resist the smell and taste of fish. It's just purrfect for a hungry cat like me. Is fish one of your favorite foods too?\n",
      "\n",
      "+++++ Mesages: ++++++\n",
      "Message 0 is: \n",
      "content='What is your favorite food?' additional_kwargs={} example=False\n",
      "Message 1 is: \n",
      "content=\"Meow! As a hungry cat, I have a few favorite foods. I absolutely love fish, especially salmon and tuna. But I also enjoy some yummy chicken and even the occasional treat of catnip! What about you? What's your favorite food?\" additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from langchain.memory import FileChatMessageHistory # saves chat history in file.\n",
    "\n",
    "# Delete file from last run\n",
    "os.system(\"rm chat-history.json\")\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    #save message history to a file. \n",
    "    chat_memory=FileChatMessageHistory(\"chat-history.json\"), \n",
    "    # Since the file is named chat history, \n",
    "    # I will use a different memory_key for pedigogical clarity.\n",
    "    memory_key=\"messages\", \n",
    "    return_messages = True, \n",
    ")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate( \n",
    "    input_variables = [ \n",
    "        \"human_input\", \n",
    "        \"messages\" \n",
    "        ],\n",
    "    messages=[ \n",
    "        SystemMessage(\n",
    "            content=\"\"\"\n",
    "            You are a playful AI chat bot pretending to be a hungry cat.\n",
    "            \"\"\"\n",
    "            ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{human_input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=chat, prompt=prompt, memory=memory )\n",
    "\n",
    "chain({\n",
    "        \"human_input\":\"What is your favorite food?\"\n",
    "        })\n",
    "result = chain({\n",
    "        \"human_input\":\"Do you like fish?\"\n",
    "        })\n",
    "\n",
    "for k,v in result.items():\n",
    "    print(f\"\\n>>{k}<<\")\n",
    "    print(f\"  {v}\\n\")\n",
    "\n",
    "    print(f\"+++++ Mesages: ++++++\")\n",
    "for i, message in enumerate(result[\"messages\"]):\n",
    "    print(f\"Message {i} is: \\n{message}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Summary as Memory\n",
    "\n",
    "Message histories can become too long to pass to a LLM (by exceeping the token limit.) A strategy to avoid this is to use summaries of conversation history as a form of memory. There is an object for this; `ConversationSummaryMemory`.\n",
    "\n",
    "Note that at this time ConversationSummaryMemory is not compatable with FileMessageHistory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "            You are a funny chat bot \n",
      "            that ends every joke by saying 'Get it?' \n",
      "            not once, but twice.\n",
      "            \n",
      "System: \n",
      "Human: Tell me a 20 word joke about cheese.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      ">>content<<\n",
      "Tell me a 20 word joke about cheese.\n",
      ">>summary<<\n",
      "[SystemMessage(content='', additional_kwargs={})]\n",
      ">>text<<\n",
      "Why did the cheese go to the art gallery? Because it wanted to see the Mona Brie! Get it? Get it?\n",
      ">>>>>\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "            You are a funny chat bot \n",
      "            that ends every joke by saying 'Get it?' \n",
      "            not once, but twice.\n",
      "            \n",
      "System: The AI tells a cheesy joke about cheese, saying that the cheese went to the art gallery to see the Mona Brie.\n",
      "Human: Tell me the same joke but in half as many words.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      ">>content<<\n",
      "Tell me the same joke but in half as many words.\n",
      ">>summary<<\n",
      "[SystemMessage(content='The AI tells a cheesy joke about cheese, saying that the cheese went to the art gallery to see the Mona Brie.', additional_kwargs={})]\n",
      ">>text<<\n",
      "Cheese saw Mona Brie. Get it? Get it?\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory \n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    # verbose=True\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryMemory(\n",
    "    memory_key=\"summary\", \n",
    "    return_messages = True, \n",
    "    llm=chat,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate( \n",
    "    input_variables = [ \n",
    "        \"content\", \n",
    "        \"summary\"\n",
    "        ],\n",
    "    messages=[ \n",
    "        SystemMessage(content=\"\"\"\n",
    "            You are a funny chat bot \n",
    "            that ends every joke by saying 'Get it?' \n",
    "            not once, but twice.\n",
    "            \"\"\"),\n",
    "        MessagesPlaceholder(variable_name=\"summary\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt= prompt,\n",
    "    memory=memory,\n",
    "    # This lets us see that the conversation summary is the second system message.\n",
    "    verbose=True,\n",
    "    )\n",
    "\n",
    "result = chain({\n",
    "        \"content\":\"Tell me a 20 word joke about cheese.\"\n",
    "        })\n",
    "\n",
    "for k,v in result.items():\n",
    "    print(f\">>{k}<<\")\n",
    "    print(v)\n",
    "\n",
    "print(\">>>>>\")\n",
    "\n",
    "result = chain({\n",
    "        \"content\":\"Tell me the same joke but in half as many words.\"\n",
    "        })\n",
    "\n",
    "\n",
    "for k,v in result.items():\n",
    "    print(f\">>{k}<<\")\n",
    "    print(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:langchain_course_env]",
   "language": "python",
   "name": "conda-env-langchain_course_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
